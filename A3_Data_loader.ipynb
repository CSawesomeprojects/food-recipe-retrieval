{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader\n",
    "Responsible to load the data during different phases (train, test) <br /> \n",
    "Data laoder reads all files and for each recipe returns its instructions, ingredeints and 6 corresponding images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch.utils.data as data\n",
    "from PIL import Image\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "import lmdb\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loader(data.Dataset):\n",
    "\n",
    "    def default_loader(path):\n",
    "        try:\n",
    "            im = Image.open(path).convert('RGB')\n",
    "            return im\n",
    "        except:\n",
    "            return Image.new('RGB', (224, 224), 'white')\n",
    "\n",
    "    \n",
    "    def __init__(self, img_path, transform=None,\n",
    "                 loader=default_loader, square=False, data_path=None, partition=None):\n",
    "        \n",
    "        if data_path == None:\n",
    "            raise Exception('No data path specified.')\n",
    "\n",
    "        if partition is None:\n",
    "            raise Exception('Unknown partition type %s.' % partition)\n",
    "        else:\n",
    "            self.partition = partition\n",
    "\n",
    "        self.env = lmdb.open(os.path.join(data_path, partition + '_lmdb'), max_readers=1,\n",
    "        readonly=True, lock=False,readahead=False, meminit=False)\n",
    "\n",
    "        with open(os.path.join(data_path, partition + '_keys.pkl'), 'rb') as f:\n",
    "            self.ids = pickle.load(f)\n",
    "\n",
    "        self.square = square\n",
    "        self.imgPath = img_path\n",
    "        self.mismtch = 0.8\n",
    "        self.maxInst = 20    \n",
    " \n",
    "        self.transform = transform\n",
    "        self.loader = loader\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        recipId = self.ids[index]\n",
    "    \n",
    "        # we force 80 percent of them to be a mismatch\n",
    "        if self.partition == 'train':\n",
    "            match = np.random.uniform() > self.mismtch\n",
    "        elif self.partition == 'val' or self.partition == 'test':\n",
    "            match = True\n",
    "        else:\n",
    "            raise 'Partition name not well defined'\n",
    "\n",
    "        target = match and 1 or -1\n",
    "        \n",
    "        \n",
    "        with self.env.begin(write=False) as txn:\n",
    "            serialized_sample = txn.get(self.ids[index].encode())\n",
    "        sample = pickle.loads(serialized_sample,encoding='latin1')\n",
    "    \n",
    "        imgs = sample['imgs']\n",
    "        \n",
    "        # image\n",
    "        if target == 1:\n",
    "            if self.partition == 'train':\n",
    "                # We choose from all the images in a recipe\n",
    "                imgIdx = np.random.choice(range(len(imgs)))\n",
    "            else:\n",
    "                imgIdx = 0\n",
    "            \n",
    "            #get the first 4 characters of the id. the first four is enough to reach to the desired folder\n",
    "            loader_path = [imgs[imgIdx]['id'][i] for i in range(4)]\n",
    "            \n",
    "            #use it as a path \n",
    "            loader_path = os.path.join(*loader_path)\n",
    "            path = os.path.join(self.imgPath, self.partition, loader_path, imgs[imgIdx]['id'])\n",
    "\n",
    "        else:\n",
    "            # we randomly pick one non-matching image\n",
    "            all_idx = range(len(self.ids))\n",
    "            rndindex = np.random.choice(all_idx)\n",
    "            while rndindex == index:\n",
    "                rndindex = np.random.choice(all_idx)  # pick a random index\n",
    "\n",
    "            with self.env.begin(write=False) as txn:\n",
    "                serialized_sample = txn.get(self.ids[rndindex].encode())\n",
    "\n",
    "            rndsample = pickle.loads(serialized_sample,encoding='latin1')\n",
    "            rndimgs = rndsample['imgs']\n",
    "\n",
    "            if self.partition == 'train':  # if training we pick a random image\n",
    "                # We do only use the first five images per recipe during training\n",
    "                imgIdx = np.random.choice(range(min(5, len(rndimgs))))\n",
    "            else:\n",
    "                imgIdx = 0\n",
    "\n",
    "            path = self.imgPath + rndimgs[imgIdx]['id']\n",
    "            \n",
    "        # load image\n",
    "        img = self.loader(path)\n",
    "\n",
    "        if self.square:\n",
    "            img = img.resize(self.square)\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        \n",
    "        # instructions\n",
    "        instrs = sample['intrs']\n",
    "        itr_ln = len(instrs)\n",
    "        t_inst = np.zeros((self.maxInst, np.shape(instrs)[1]), dtype=np.float32)\n",
    "        t_inst[:itr_ln][:] = instrs\n",
    "        instrs = torch.FloatTensor(t_inst)\n",
    "\n",
    "        # ingredients\n",
    "        ingrs = sample['ingrs'].astype(int)\n",
    "        ingrs = torch.LongTensor(ingrs)\n",
    "        igr_ln = max(np.nonzero(sample['ingrs'])[0]) + 1\n",
    "       \n",
    "\n",
    "        rec_class = sample['classes'] - 1\n",
    "        rec_id = self.ids[index]\n",
    "\n",
    "        if target == -1:\n",
    "            img_class = rndsample['classes'] - 1\n",
    "            img_id = self.ids[rndindex]\n",
    "        else:\n",
    "            img_class = sample['classes'] - 1\n",
    "            img_id = self.ids[index]\n",
    "\n",
    "        \n",
    "    \n",
    "        category = sample['classes']\n",
    "        \n",
    "        \n",
    "        return recipId, img, instrs, ingrs, category, target\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating train and test data\n",
    "we generate the training data in a way to force 80 percent of image-recipe pairs are mismatched and only 20 percent of them are matched.  <br />\n",
    "For generating the test data, we force to create  matched recipe-image pairs.  <br />\n",
    "batch size for train and test is 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "\n",
    "params = {'batch_size': batch_size, 'shuffle':True, 'pin_memory':True}\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "\n",
    "transform = transforms.Compose([\n",
    "            transforms.Scale(256), # rescale the image keeping the original aspect ratio\n",
    "            transforms.CenterCrop(256), # we get only the center of that rescaled\n",
    "            transforms.RandomCrop(224), # random crop within the center crop \n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize\n",
    "            ])\n",
    "\n",
    "\n",
    "training_set = Loader(data_path='../data', partition = 'train', img_path='../data' , transform=transform)\n",
    "training_generator = data.DataLoader(training_set, **params)\n",
    "\n",
    "test_set = Loader(data_path='../data', partition = 'test', img_path='../data' , transform=transform)\n",
    "test_generator = data.DataLoader(test_set, **params)\n",
    "\n",
    "f = open('../data/recipe2ingredients.pkl','wb') \n",
    "\n",
    "for i, (recipeId, img, instrs, ingrs, category, target) in enumerate(training_generator): \n",
    "\n",
    "    with open('../data/recipe2ingredients.pkl'.format(recipeId[0]),'ab+') as f:\n",
    "            pickle.dump({'recipeId':recipeId[0],'ingrs':ingrs[0]},f)\n",
    "       \n",
    "    if(i==100):\n",
    "        break\n",
    "    \n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished reading file\n"
     ]
    }
   ],
   "source": [
    "recipe2ingredients = []\n",
    "with open('../data/recipe2ingredients.pkl','rb') as f:\n",
    "    while True:\n",
    "        try:\n",
    "            recipe2ingredients.append(pickle.load(f))\n",
    "        except:\n",
    "            print(\"finished reading file\")\n",
    "            break\n",
    "    \n",
    "    #ingrs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample of ingredient vector for one Recipe\n",
    "id is 99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'recipeId': '3dd0b17c87', 'ingrs': tensor([ 5824,  4514,  6595, 15206,   839,  1704,  3913,   466,  5552,  5393,\n",
      "        27698,  1491, 22391,  2456, 27804,  3646, 17791,     1,     0,     0])}\n"
     ]
    }
   ],
   "source": [
    "print(recipe2ingredients[99])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
